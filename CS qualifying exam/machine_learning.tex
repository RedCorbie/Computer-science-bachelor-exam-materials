\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[fleqn]{amsmath}
\usepackage{amsfonts, amssymb, amsthm, mathtools}

\usepackage{mathtools}
\usepackage{fullpage}
\usepackage[colorinlistoftodos]{todonotes}


\title{Машинное обучение}
\author{MIPT DIHT}


\theoremstyle{plain}
\newtheorem*{theorem-star}{Theorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{lem-star}{Lemma}
\newtheorem{lem}{Lemma}
\newtheorem*{proposition-star}{Proposition}
\newtheorem{proposition}{Proposition}
\newtheorem{statement}{Statement}
\newtheorem*{statement-star}{Statement}
\newtheorem{corollary}{Corollary}
\newtheorem*{corollary-star}{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\theoremstyle{definition}
\newtheorem*{definition-star}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem*{example-star}{Example}
\newtheorem{problem}{Problem}
\newtheorem{problem-star}{Problem}

\renewenvironment{proof}{{\bfseries Proof}}{$\bullet$}

\newcommand{\myequat}[1]{\begin{equation} #1 \nonumber \end{equation}}
\newcommand{\pars}[1]{\left( #1 \right)} 
\newcommand{\class}[1]{\left[ #1 \right]} 
\newcommand{\dd}{\; \mathrm{d}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setRn}{\mathbb{R}^n}
\newcommand{\setRinf}{\mathbb{R}^{\infty}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setM}{\mathcal{M}}
\newcommand{\setL}{\mathcal{L}}
\newcommand{\setA}{\mathcal{A}}
\newcommand{\setF}{\mathcal{F}}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\newcommand{\walls}[1]{\left | #1 \right |} % |smth_vertically_large|
\newcommand{\braces}[1]{\left\{ #1 \right\}} % {smth_vertically_large}

\newcommand{\condset}[2]{\braces{\, #1 \mid #2 \,}} % definition of set with condition

\newcommand{\expl}[1]{\walls{\text{#1}}} % explanation inside formula

\newcommand{\toup}[1]{\xrightarrow{#1}}
\newcommand{\toae}{\toup{\text{\,п.н.}}} % almost everywhere convergence designation
\newcommand{\todown}[1]{\xrightarrow[#1]{}}

\newcommand{\equp}[1]{\stackrel{#1}{=}}

\newcommand{\conj}[1]{\overline{#1}} % complex conjugation
\newcommand{\comp}[1]{\overline{#1}} % set complement

\DeclareMathOperator{\cov}{cov}

\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\Im}{\mathop{\mathrm{Im}}\nolimits}
\renewcommand{\Re}{\mathop{\mathrm{Re}}\nolimits}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\bigtitle}[1]{\title{\textbf{\underline{#1}}}}
\newcommand{\boldtitle}[1]{\title{\textbf{#1}}}

\newcommand*{\hm}[1]{#1\nobreak\discretionary{}%
{\hbox{$\mathsurround=0pt #1$}}{}} % a\hm=b makes "=" carriable to the next line with duplication of the sign


\begin{document}
\maketitle

\section{Билет №1}
\subsection{Байесовские методы классификации. Наивный байесовский классификатор}
\bigtitle{Постановка задачи}
Пусть X — множество объектов, Y — конечное множество имён классов, множество $X \times Y$ является вероятностным пространством с плотностью распределения $p(x, y) = P(y)p(x|y)$. Вероятности появления объектов каждого из классов $P_y = P(y)$ называются априорными вероятностями классов. Плотности распределения $p_y(x) = p(x|y)$ называются функциями правдоподобия классов. Вероятностная постановка задачи классификации разделяется на две независимые подзадачи. 
\begin{problem}Имеется простая выборка $X^l = (x_i, y_i)_{i=1}^l$ из неизвестного распределения $p(x, y) = P_yp_y(x)$. Требуется построить эмпирические оценки априорных вероятностей $\hat{P}_y$ и функций правдоподобия $\hat{p}_y(x)$ для каждого из классов $y \in Y$ .
\end{problem}
\begin{problem} По известным плотностям распределения $p_y(x)$ и априорным вероятностям $P_y$ всех классов $y \in Y$ построить алгоритм a(x), минимизирующий вероятность ошибочной классификации.
\end{problem}
Первая задача имеет множество решений, поскольку многие распределения $p(x, y)$ могли бы породить одну и ту же выборку $X^l$

\bigtitle{Функционал среднего риска}
Событие вида «$x \in \Omega$ при условии, что x принадлежит классу y»:
$$ P(\Omega|y) = \int_{\Omega} p_y(x) dx $$
\begin{definition} Функционалом среднего риска называется ожидаемая величина потери
при классификации объектов алгоритмом a: $$ R(a) = \sum_{\substack{y \in Y}} \sum_{\substack{s \in Y}} \lambda_{ys} P_y P(a_s|y)$$\end{definition}

\bigtitle{Оптимальное байесовское решающее правило}
\begin{theorem}
Если известны априорные вероятности $P_y$ и функции правдоподобия $p_y(x)$, то минимум среднего риска R(a) достигается алгоритмом
$$a(x) = arg \min_{\substack{s \in Y}} \sum_{\substack{y \in Y}} \lambda_{ys} P_y p_y(x) $$
\end{theorem}
\begin{proof} Для произвольного t из Y запишем функционал среднего риска:
$$ R(a) = \sum_{\substack{y \in Y}} \sum_{\substack{s \in Y}}\lambda_{ys} P_y P(A_s|y) = $$
$$ = \sum_{\substack{y \in Y}}\lambda_{ys} P_y P(A_t|y) + \sum_{\substack{s \in Y\\{t}}} \sum_{\substack{y \in Y}}\lambda_{ys} P_y P(A_s|y)  $$
Из формулы полной вероятности $ P(A_t|y) = 1- \sum_{\substack{s \in Y\\{t}}}P(A_s|y) $
$$ R(a) = \sum_{\substack{y \in Y}}\lambda_{ys} P_y + \sum_{\substack{s \in Y\\{t}}} \sum_{\substack{y \in Y}}(\lambda_{ys} - \lambda_{yt}) P_y P(a_s|y) = $$
$$ = const(a) +  \sum_{\substack{s \in Y\\{t}}} \int_{A_s} \sum_{\substack{y \in Y}}(\lambda_{ys} - \lambda_{yt}) P_y p_y(x)$$
Обозначим $ g_s(x) =  \sum_{\substack{y \in Y}}\lambda_{ys} P_y p_y(x)$
$$ R(a) = const(a) +  \sum_{\substack{s \in Y\\{t}}} \int_{A_s} (g_s(x) - g_t(x))dx $$
В выражении неизвестны только области $A_s$. Функционал R(a) есть сумма $|Y|-1$ слагаемых $I(A_s) = \int_{A_s}(g_s(x) - g_t(x)dx$, каждое из которых зависит только от одной области $A_s$. Минимум $I(A_s)$ достигается, когда $A_s$ совпадает с областью неположительности подынтегрального выражения. В силу произвольности t:
$$ A_s = \{x \in X|g_s(x) \leq _t(x), \forall t \in Y, t \neq s\} $$
С другой стороны $a_s = \{x \in X| a(x)=s\}$. Значит $a(x) = s \iff s = arg \min_{\substack{t \in Y}} g_t(x)$
\end{proof}
\begin{theorem}
Если $P_y$ и $p_y(x)$ известны, $\lambda_{ss}=0, \lambda_{ys} \equiv \lambda_y \forall y,s \in Y$, то минимум среднего риска достигается алгоритмом
$$ a(x) = arg \max_{\substack{y \in Y}} \lambda_y P_y p_y(x) $$
\end{theorem}
\bigtitle{Наивный байесовский классификатор}
\begin{problem} Задано множество объектов $X^m = {x_1, \ldots , x_m}$, выбранных случайно и независимо согласно неизвестному распределению p(x). Требуется построить эмпирическую оценку плотности — функцию $\hat{p}(x)$, приближающую p(x) на всём X.
Допустим, что объекты $x \in X$ описываются n числовыми признаками $f_j: X \rightarrow R, j = 1, \ldots, n$. Обозначим через $x =
= (\xi_1, . . . , \xi_n)$ произвольный элемент пространства объектов $X = \setR^n$, где $\xi_j = f_j (x)$.
\end{problem}
\begin{proposition}
Признаки $f_1(x),\ldots, f_n(x)$ являются независимыми случайными величинами. Следовательно, функции правдоподобия классов представимы в виде $p_y(x) = p_{y1}(\xi_1)· · · p_{yn}(\xi_n), y \in Y$, где $p_{yj} (\xi_j )$ — плотность распределения значений j-го признака для класса y.
\end{proposition}
Подставим эмпирические оценки одномерных плотностей $\hat{p}_{yj} (\xi)$ в предположение и затем в теорему 2. Получим алгоритм
$$ a(x) = arg \max_{\substack{y \in Y}} \bigg( ln \lambda_y \hat{P}_y + \sum){\substack{j=1}}^n ln \hat{p}_{yj}(\xi_j) \bigg) $$

\section{Билет №2}
\subsection{Логистическая регрессия}
Базовые предположения. Пусть классов два, $Y = {-1, +1}$, объекты описываются n числовыми признаками $f_j: X \rightarrow R, j = 1, \ldots , n$. Будем полагать $X = \setR^n$ отождествляя объекты с их признаковыми описаниями: $x \equiv (f_1(x), \ldots , f_n(x))$.
\subsection{L1 и L2 регуляризации}

\subsection{Теорема об оптимальности}

\section{Билет №3}
\subsection{Метод опорных векторов. Решение двойственной задачи}
\subsection{Спрямляющее пространство}
\subsection{Ядра}

\section{Билет №4}
\subsection{Многомерная линейная регрессия}
\bigtitle{Метод наименьших квадратов}
$$ Q(\alpha, X^l) = \sum_{\substack{i=1}}^l w_i \Big( f(x_i, \alpha) - y_i)^2 \Big) \rightarrow \min_{\substack{\alpha}} $$
где $w_i$ - вес i-го объекта
$Q(\alpha^*, X^l$ - остаточная сумма квадратов
\bigtitle{Многомерная логрегрессия}
$f_1(x) \ldots f_n(x)$ - числовые признаки. Модель многомерной регрессии:
$$f(x, \alpha) = \sum_{\substack{j=1}}^n \alpha_j f_j(x), \alpha \in \setR^n$$
Функионал квадрата ошибки:
$$ Q(\alpha, X^l) = \sum_{\substack{i=1}}^l (f(x_i, \alpha) - y_i)^2 = ||F\alpha - y ||^2 \rightarrow \min_{\substack{\alpha}} $$
Необхожимое условие минимума в матричном виде: $$ \frac{\partial Q}{\partial \alpha}(\alpha) = 2F^T(F\alpha-y) = 0 $$
откуда следует нормальная система задачи метода наименьших квадратов: $F^TF\alpha = F^Ty$
Решение системы: $\alpha^* = (F^TF)^{-1}F^Ty = F^{+}y$. Значение функционала: $Q(\alpha^*) = ||P_Fy -y ||^2$, где $P_F = FF^+ = (F^TF)^{-1}F^T$ - проекционная матрица
\subsection{Лассо Тибширани}
$$
\begin{cases}
	Q(\alpha) = ||F\alpha - y||^2 \rightarrow \min_{\substack{\alpha}}, \\
	\sum_{\substack{j=1}}^n|\alpha_j| \leq \varkappa
\end{cases}
$$
Лассо приводит к отбору признаков. После замены переменных
$$ \begin{cases}
	\alpha_j = \alpha_j^+ - \alpha_j^- \\
	|\alpha_j| = \alpha_j^+ + \alpha_j^- 
\end{cases}, \alpha_j^+, \alpha_j^- \geq 0 $$
ограничения принимают канонический вид $$ \sum_{\substack{j=1}}^n \alpha_j^+ + \alpha_j^- \leq \varkappa $$
Чем меньше $\varkappa$, тем больше таких j что $\alpha_j^+ = \alpha_j^- =0$

\subsection{Гребневая регрессия}
Штраф за увеличение норма вектора весов $||\alpha||$:
$$ Q_\tau(\alpha) = ||F\alpha - y||^2 + \frac{1}{\sigma} ||\alpha||^2$$
где $\tau = \frac{1}{\sigma}$ - неотрицательный параметр регуляризации.
\bigtitle{Вероятностная интерпретация}
Априорное распределение вектора $\alpha$ - гауссовское с ковариационной матрицей $\sigma I_n$
Модифицированное МНК-решение: $$ \alpha_\tau^* = (F^{T}F + \tau I_n)^{-1}F^{T}y $$


\section{Билет №5}
\subsection{Бустинг}
\subsection{Алгоритм AdaBoost}

\end{document}