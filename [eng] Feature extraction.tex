\documentclass[11pt]{article}
\usepackage{fontspec} 
\usepackage{polyglossia}
\setmainlanguage{russian} 
\setotherlanguage{english}
\newfontfamily{\cyrillicfont}{Times New Roman}

\usepackage{mathtools}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage[colorinlistoftodos]{todonotes}


\title{Feature extraction}
\author{IITP PHM course}
\begin{document}
\maketitle

\section{Problem}

$X_1 \ldots X_n, X_i \in R^{m}$ \\

\textbf{Supervised: } 
$y_i \in {C_1 \ldots C_k}$ \\

\subsection{Feature types}
\begin{enumerate}
\item Flat features
\item Streaming features
\item Structured features
\end{enumerate}


\section{Methods of supervised learning}

Can be used for flat and scructured features: \\
\begin{enumerate}
\item Filter
\item Wrapper: selectinon features based on current algorithm and checking on existing objects
\item Embedded models: during building classification add regularization
\end{enumerate}

\subsection{Filters}

\textbf{$\bullet$ Fisher score: }\\

$$S = \frac{\sum_{k=1}^K n_j(\mu_{ij} - \mu_i)^2}{\sum_{k=1}^K n_k \rho_ij^2} $$ \\

$\mu_{ij}$ and $\rho_{ij}$ - mean and dispersion of i-th feature in j-th class \\

\textbf{$\bullet$ Mutual information} \\

$$IG(f_i, C) = H(f_i) - H(f_i|C)$$ \\
$$H(f_i) = - \sum_{j} p(x_{ij}) \log_{2}(p(x_i))$$ \\
$$H(f_i|C) = - \sum_{k} p(C_k)\sum_{j} p(x_j|C_k) \log_{2}(p(x_j|C_k))$$ \\

Symmetrical uncertainty: \\

$$SU = 2 \frac{IG(f_i, C)}{H(f_i)+H(C)}$$ \\
$$SU(f_i, C_k) \geq \rho $$ \\

\textbf{$\bullet$ Relief} \\
In case of two classes: \\
l = random subsample \\
$$S_i = \frac{1}{2} \sum_{k=1}^l d(x_{ik} - x_{iM_k}) - d(x_ik - x_{iH_k})$$ \\
$\bullet M_k$ - i-th feature of k nearest neighbours from same class \\
$\bullet H_k$ - i-th feature of k nearest neighbours from another class \\

\subsection{Wrappers} 
Idea: grid algorithms \\

\subsection{Embedded models}

$$c(k, X, y) + \alpha*penalty(\omega) $$ \\
$c$ is loss function\\
$c - Hinge loss: \sum_{i=1}^n \max(0, 1-y_i \omega^{\top} x_i)$ \\
$c - logistic loss: \sum_{i=1}^n \log (1+exp(-y_i(\omega^{\top}x_i + b))) $ \\

\textbf{Penalty:}
\begin{enumerate}
\item $L_1: pen(\omega) = \sum \mid \omega_j \mid$
\item $L_2: pen(\omega) = \sum \omega_j^{2}$
\item ElasticNet: $\beta \sum \mid \omega_j \mid + (1-\beta) \sum \omega_j^{2} $
\end{enumerate}

\subsection{Lasso and GroupLasso}
$\bullet$ GroupLasso \\
$$pen(\omega) = \sum_{i=1}^k h_i \parallel W_{G_i} \parallel $$
$W = (W_{G_1} \ldots W_{G_k})$ \\

\textbf{Note:} feature splitting could be tree-based: $G_1^{0} \rightarrow (G_1^{1}, G_2^{1}, G_3^{1}) \rightarrow (G_1^{2}, (G_2^{2}, G_3^{2}), G_3^{1})$

\end{document}